{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in ./tf/lib/python3.10/site-packages (0.14.4)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in ./tf/lib/python3.10/site-packages (from statsmodels) (1.26.4)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in ./tf/lib/python3.10/site-packages (from statsmodels) (1.15.2)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in ./tf/lib/python3.10/site-packages (from statsmodels) (2.2.3)\n",
      "Requirement already satisfied: packaging>=21.3 in ./tf/lib/python3.10/site-packages (from statsmodels) (24.2)\n",
      "Requirement already satisfied: patsy>=0.5.6 in ./tf/lib/python3.10/site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./tf/lib/python3.10/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./tf/lib/python3.10/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./tf/lib/python3.10/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./tf/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
      "Model Evaluation (ARIMA) for 254: MAPE: 29.00%\n",
      "        DATE  Predicted NetAmount\n",
      "0 2025-01-01        173583.292294\n",
      "1 2025-01-02        171408.384007\n",
      "2 2025-01-03        179209.143717\n",
      "3 2025-01-04        191182.312598\n",
      "4 2025-01-05        178874.497916\n",
      "5 2025-01-06        181174.418667\n",
      "6 2025-01-07        179734.586818\n"
     ]
    }
   ],
   "source": [
    "!pip install statsmodels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "data = pd.read_csv('2023-2024-Horana-cleaned.csv') \n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "data['NetAmount'] = data['NetAmount'].abs()\n",
    "\n",
    "daily_sales = data.groupby(['DATE', 'SalesPersonCode'])['NetAmount'].sum().reset_index()\n",
    "\n",
    "def evaluate_model_daily(sales_code):\n",
    "    person_data = daily_sales[daily_sales['SalesPersonCode'] == sales_code]\n",
    "    person_data = person_data.set_index('DATE').asfreq('D').fillna(0)  # Ensure daily frequency\n",
    "\n",
    "    train = person_data.iloc[:-7]  # Training data\n",
    "    test = person_data.iloc[-7:]   # Last 7 days for evaluation\n",
    "\n",
    "    try:\n",
    "        model = ARIMA(train['NetAmount'], order=(5, 1, 0))\n",
    "        model_fit = model.fit()\n",
    "        forecast = model_fit.forecast(steps=7)\n",
    "\n",
    "        results_df = pd.DataFrame({\n",
    "            'DATE': pd.date_range(start=test.index[-1] + pd.Timedelta(days=1), periods=7, freq='D'),\n",
    "            'Predicted NetAmount': forecast.values\n",
    "        })\n",
    "\n",
    "        mape = mean_absolute_percentage_error(test['NetAmount'], forecast) * 100\n",
    "        print(f\"Model Evaluation (ARIMA) for {sales_code}: MAPE: {mape:.2f}%\")\n",
    "\n",
    "        if mape > 40:\n",
    "            print(f\"High MAPE ({mape:.2f}%). Trying ETS model for {sales_code}.\")\n",
    "            return evaluate_ets_daily(sales_code, test)\n",
    "\n",
    "        return results_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ARIMA failed for {sales_code} due to {e}. Switching to ETS...\")\n",
    "        return evaluate_ets_daily(sales_code, test)\n",
    "\n",
    "def evaluate_ets_daily(sales_code, test):\n",
    "    person_data = daily_sales[daily_sales['SalesPersonCode'] == sales_code]\n",
    "    \n",
    "    if len(person_data) < 10:\n",
    "        print(f\"Not enough data for {sales_code}, using mean-based prediction.\")\n",
    "        mean_forecast = [person_data['NetAmount'].mean()] * 7\n",
    "        return pd.DataFrame({'DATE': pd.date_range(start=test.index[-1] + pd.Timedelta(days=1), periods=7, freq='D'),\n",
    "                             'Predicted NetAmount': mean_forecast})\n",
    "\n",
    "    train = person_data.iloc[:-7]\n",
    "\n",
    "    model = ExponentialSmoothing(train['NetAmount'], trend=\"add\", seasonal=None)\n",
    "    model_fit = model.fit()\n",
    "    forecast = model_fit.forecast(steps=7)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'DATE': pd.date_range(start=test.index[-1] + pd.Timedelta(days=1), periods=7, freq='D'),\n",
    "        'Predicted NetAmount': forecast.values\n",
    "    })\n",
    "\n",
    "    mape = mean_absolute_percentage_error(test['NetAmount'], forecast) * 100\n",
    "    print(f\"Model Evaluation (ETS) for {sales_code}: MAPE: {mape:.2f}%\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "sales_code = \"254\"  \n",
    "forecast_results = evaluate_model_daily(sales_code)\n",
    "print(forecast_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation (ARIMA) for 265: MAPE: 34.30%\n",
      "ARIMA failed for 265 due to tuple index out of range. Switching to ETS...\n",
      "Model Evaluation (ETS) for 265: MAPE: 33.24%\n",
      "        DATE  Predicted NetAmount\n",
      "0 2025-01-01        149007.728762\n",
      "1 2025-01-02        151650.037461\n",
      "2 2025-01-03        154292.346159\n",
      "3 2025-01-04        156934.654858\n",
      "4 2025-01-05        159576.963557\n",
      "5 2025-01-06        162219.272255\n",
      "6 2025-01-07        164861.580954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pasiya/sales_forcast/jupytercode/tf/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: An unsupported index was provided. As a result, forecasts cannot be generated. To use the model for forecasting, use one of the supported classes of index.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/pasiya/sales_forcast/jupytercode/tf/lib/python3.10/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n",
      "/home/pasiya/sales_forcast/jupytercode/tf/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "/home/pasiya/sales_forcast/jupytercode/tf/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "data = pd.read_csv('2023-2024-Horana-cleaned.csv') \n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "data['NetAmount'] = data['NetAmount'].abs()\n",
    "\n",
    "daily_sales = data.groupby(['DATE', 'SalesPersonCode'])['NetAmount'].sum().reset_index()\n",
    "\n",
    "def evaluate_model_daily(sales_code):\n",
    "    person_data = daily_sales[daily_sales['SalesPersonCode'] == sales_code]\n",
    "    person_data = person_data.set_index('DATE').asfreq('D').fillna(0)  # Ensure daily frequency\n",
    "\n",
    "    train = person_data.iloc[:-7] \n",
    "    test = person_data.iloc[-7:]   \n",
    "\n",
    "    try:\n",
    "        model = ARIMA(train['NetAmount'], order=(5, 1, 0))\n",
    "        model_fit = model.fit()\n",
    "        forecast = model_fit.forecast(steps=7)\n",
    "\n",
    "        results_df = pd.DataFrame({\n",
    "            'DATE': pd.date_range(start=test.index[-1] + pd.Timedelta(days=1), periods=7, freq='D'),\n",
    "            'Predicted NetAmount': forecast.values\n",
    "        })\n",
    "\n",
    "        mape = mean_absolute_percentage_error(test['NetAmount'], forecast) * 100\n",
    "        print(f\"Model Evaluation (ARIMA) for {sales_code}: MAPE: {mape:.2f}%\")\n",
    "\n",
    "        if mape > 40:\n",
    "            print(f\"High MAPE ({mape:.2f}%). Trying ETS model for {sales_code}.\")\n",
    "            return evaluate_ets_daily(sales_code, test)\n",
    "\n",
    "        generate_shap_report(model_fit, train, test)\n",
    "\n",
    "        return results_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ARIMA failed for {sales_code} due to {e}. Switching to ETS...\")\n",
    "        return evaluate_ets_daily(sales_code, test)\n",
    "\n",
    "def evaluate_ets_daily(sales_code, test):\n",
    "    person_data = daily_sales[daily_sales['SalesPersonCode'] == sales_code]\n",
    "    \n",
    "    if len(person_data) < 10:\n",
    "        print(f\"Not enough data for {sales_code}, using mean-based prediction.\")\n",
    "        mean_forecast = [person_data['NetAmount'].mean()] * 7\n",
    "        return pd.DataFrame({'DATE': pd.date_range(start=test.index[-1] + pd.Timedelta(days=1), periods=7, freq='D'),\n",
    "                             'Predicted NetAmount': mean_forecast})\n",
    "\n",
    "    train = person_data.iloc[:-7]\n",
    "\n",
    "    model = ExponentialSmoothing(train['NetAmount'], trend=\"add\", seasonal=None)\n",
    "    model_fit = model.fit()\n",
    "    forecast = model_fit.forecast(steps=7)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'DATE': pd.date_range(start=test.index[-1] + pd.Timedelta(days=1), periods=7, freq='D'),\n",
    "        'Predicted NetAmount': forecast.values\n",
    "    })\n",
    "\n",
    "    mape = mean_absolute_percentage_error(test['NetAmount'], forecast) * 100\n",
    "    print(f\"Model Evaluation (ETS) for {sales_code}: MAPE: {mape:.2f}%\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "def generate_shap_report(model_fit, train, test):\n",
    "    explainer = shap.Explainer(model_fit.predict, train['NetAmount'])\n",
    "    shap_values = explainer(test['NetAmount'])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, show=False)\n",
    "    plt.title(\"SHAP Report for ARIMA Model\")\n",
    "    plt.show()\n",
    "\n",
    "sales_code = \"265\"  \n",
    "forecast_results = evaluate_model_daily(sales_code)\n",
    "print(forecast_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pasiya/sales_forcast/jupytercode/tf/lib/python3.10/site-packages/shap/explainers/_deep/deep_tf.py:99: UserWarning: Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n",
      "  warnings.warn(\"Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\")\n",
      "/home/pasiya/sales_forcast/jupytercode/tf/lib/python3.10/site-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor\n",
      "Received: inputs=['Tensor(shape=(9, 12, 1))']\n",
      "  warnings.warn(msg)\n",
      "/home/pasiya/sales_forcast/jupytercode/tf/lib/python3.10/site-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor\n",
      "Received: inputs=['Tensor(shape=(18, 12, 1))']\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "StagingError",
     "evalue": "in user code:\n\n    File \"/home/pasiya/sales_forcast/jupytercode/tf/lib/python3.10/site-packages/shap/explainers/_deep/deep_tf.py\", line 269, in grad_graph  *\n        x_grad = tape.gradient(out, shap_rAnD)\n\n    LookupError: gradient registry has no entry for: shap_TensorListStack\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStagingError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mDeepExplainer(model, X_train)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate SHAP values for the test set\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Plot SHAP values for the first prediction\u001b[39;00m\n\u001b[1;32m     10\u001b[0m shap\u001b[38;5;241m.\u001b[39minitjs()\n",
      "File \u001b[0;32m~/sales_forcast/jupytercode/tf/lib/python3.10/site-packages/shap/explainers/_deep/__init__.py:159\u001b[0m, in \u001b[0;36mDeepExplainer.shap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshap_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, ranked_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_rank_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, check_additivity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return approximate SHAP values for the model applied to the data given by X.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranked_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_rank_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_additivity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_additivity\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sales_forcast/jupytercode/tf/lib/python3.10/site-packages/shap/explainers/_deep/deep_tf.py:329\u001b[0m, in \u001b[0;36mTFDeep.shap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# run attribution computation graph\u001b[39;00m\n\u001b[1;32m    328\u001b[0m feature_ind \u001b[38;5;241m=\u001b[39m model_output_ranks[j,i]\n\u001b[0;32m--> 329\u001b[0m sample_phis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphi_symbolic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_ind\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# assign the attributions to the right part of the output arrays\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X)):\n",
      "File \u001b[0;32m~/sales_forcast/jupytercode/tf/lib/python3.10/site-packages/shap/explainers/_deep/deep_tf.py:385\u001b[0m, in \u001b[0;36mTFDeep.run\u001b[0;34m(self, out, model_inputs, X)\u001b[0m\n\u001b[1;32m    382\u001b[0m         tf_execute\u001b[38;5;241m.\u001b[39mrecord_gradient \u001b[38;5;241m=\u001b[39m tf_backprop\u001b[38;5;241m.\u001b[39mrecord_gradient\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_out\n\u001b[0;32m--> 385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_with_overridden_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43manon\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sales_forcast/jupytercode/tf/lib/python3.10/site-packages/shap/explainers/_deep/deep_tf.py:420\u001b[0m, in \u001b[0;36mTFDeep.execute_with_overridden_gradients\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# define the computation graph for the attribution values using a custom gradient-like computation\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# reinstate the backpropagatable check\u001b[39;00m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tf_gradients_impl, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_IsBackpropagatable\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/sales_forcast/jupytercode/tf/lib/python3.10/site-packages/shap/explainers/_deep/deep_tf.py:378\u001b[0m, in \u001b[0;36mTFDeep.run.<locals>.anon\u001b[0;34m()\u001b[0m\n\u001b[1;32m    376\u001b[0m     v \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(data, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_inputs[i]\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    377\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mappend(v)\n\u001b[0;32m--> 378\u001b[0m final_out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     tf_execute\u001b[38;5;241m.\u001b[39mrecord_gradient \u001b[38;5;241m=\u001b[39m tf_backprop\u001b[38;5;241m.\u001b[39m_record_gradient\n",
      "File \u001b[0;32m~/sales_forcast/jupytercode/tf/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/sales_forcast/jupytercode/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:52\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStagingError\u001b[0m: in user code:\n\n    File \"/home/pasiya/sales_forcast/jupytercode/tf/lib/python3.10/site-packages/shap/explainers/_deep/deep_tf.py\", line 269, in grad_graph  *\n        x_grad = tape.gradient(out, shap_rAnD)\n\n    LookupError: gradient registry has no entry for: shap_TensorListStack\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.DeepExplainer(model, X_train)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][0], X_test[0], feature_names=[\"NetAmount\"])\n",
    "\n",
    "shap.summary_plot(shap_values, X_test, feature_names=[\"NetAmount\"])\n",
    "\n",
    "shap.dependence_plot(0, shap_values[0], X_test, feature_names=[\"NetAmount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
