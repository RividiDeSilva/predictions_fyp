{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55562dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0618d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b4f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('2023-2024-Horana-cleaned.csv') \n",
    "\n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "\n",
    "data['NetAmount'] = data['NetAmount'].abs()\n",
    "\n",
    "data['SalesPersonCode']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ac6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_year = data['DATE'].dt.year.max()\n",
    "\n",
    "# Filter the data to include only the last year\n",
    "last_year_data = data[data['DATE'].dt.year == last_year]\n",
    "\n",
    "valid_sales_persons = last_year_data['SalesPersonCode'].unique()\n",
    "\n",
    "filtered_data = data[data['SalesPersonCode'].isin(valid_sales_persons)]\n",
    "\n",
    "filtered_data.to_csv('filtered_Horana_cleaned.csv', index=False)\n",
    "\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d521a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow\n",
    "!pip install tensorflow==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca1dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89901ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39a67cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# def dummy_npwarn_decorator_factory():\n",
    "#   def npwarn_decorator(x):\n",
    "#     return x\n",
    "#   return npwarn_decorator\n",
    "# np._no_nep50_warning = getattr(np, '_no_nep50_warning', dummy_npwarn_decorator_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e414be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense\n",
    "# import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f26301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy 2.0.4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==2.?1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de8273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be3f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install shap\n",
    "# import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df2169",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('filtered_Horana_cleaned.csv')\n",
    "\n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "\n",
    "daily_sales = data.groupby(['DATE', 'SalesPersonCode'])['NetAmount'].sum().reset_index()\n",
    "\n",
    "pivot_data = daily_sales.pivot(index='DATE', columns='SalesPersonCode', values='NetAmount').fillna(0)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(pivot_data)\n",
    "\n",
    "def create_dataset(data, time_step=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step - 1):\n",
    "        X.append(data[i:(i + time_step), :])\n",
    "        y.append(data[i + time_step, :])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_step = 30  \n",
    "X, y = create_dataset(scaled_data, time_step)\n",
    "\n",
    "X = X.reshape(X.shape[0], X.shape[1], X.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7160d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(time_step, X.shape[2])))\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dense(X.shape[2]))  \n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X, y, epochs=20, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b311ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_30_days = scaled_data[-time_step:]  \n",
    "predictions = []\n",
    "\n",
    "for _ in range(30):  \n",
    "    last_30_days_reshaped = last_30_days.reshape(1, time_step, X.shape[2])\n",
    "    next_day_pred = model.predict(last_30_days_reshaped)\n",
    "    predictions.append(next_day_pred[0])\n",
    "    last_30_days = np.vstack([last_30_days[1:], next_day_pred])\n",
    "\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "prediction_dates = pd.date_range(start=pivot_data.index[-1] + pd.Timedelta(days=1), periods=30)\n",
    "prediction_df = pd.DataFrame(predictions, columns=pivot_data.columns, index=prediction_dates)\n",
    "\n",
    "prediction_df.to_csv('upcoming_month_sales_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_upcoming_month_sales(sales_person_code):\n",
    "    last_30_days = scaled_data[-time_step:]  \n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(30): \n",
    "        last_30_days_reshaped = last_30_days.reshape(1, time_step, X.shape[2])\n",
    "        next_day_pred = model.predict(last_30_days_reshaped)\n",
    "        predictions.append(next_day_pred[0])\n",
    "        last_30_days = np.vstack([last_30_days[1:], next_day_pred])\n",
    "\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "    prediction_dates = pd.date_range(start=pivot_data.index[-1] + pd.Timedelta(days=1), periods=30)\n",
    "    prediction_df = pd.DataFrame(predictions, columns=pivot_data.columns, index=prediction_dates)\n",
    "\n",
    "    total_sales = prediction_df[sales_person_code].sum()\n",
    "    return total_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766efa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_person_code = '265'\n",
    "total_predicted_sales = predict_upcoming_month_sales(sales_person_code)\n",
    "print(f\"Total predicted sales for SalesPersonCode {sales_person_code} in the upcoming month: {total_predicted_sales:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb88082",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(scaled_data) * 0.8)\n",
    "train_data, test_data = scaled_data[:train_size], scaled_data[train_size - time_step:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae40053",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea369059",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_scaled = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3628fc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_original = scaler.inverse_transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0621bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "mse = mean_squared_error(y_test_original, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test_original, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0801a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "data = pd.read_csv('2023-2024-Horana-cleaned.csv')\n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "data['NetAmount'] = data['NetAmount'].abs()\n",
    "\n",
    "sales_265 = data[data['SalesPersonCode'] == '0']\n",
    "\n",
    "sales_265_monthly = sales_265.resample('M', on='DATE')['NetAmount'].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(sales_265_monthly['DATE'], sales_265_monthly['NetAmount'], marker='o', linestyle='-')\n",
    "plt.title('Monthly Sales Trend for SalesPersonCode 254')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e046e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(series):\n",
    "    result = adfuller(series, autolag='AIC')\n",
    "    print(\"ADF Test Results:\")\n",
    "    print(f\"Test Statistic: {result[0]}\")\n",
    "    print(f\"P-Value: {result[1]}\")\n",
    "    print(f\"Critical Values: {result[4]}\")\n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Data is stationary (reject H0)\")\n",
    "    else:\n",
    "        print(\"Data is non-stationary (fail to reject H0)\")\n",
    "\n",
    "adf_test(sales_265_monthly['NetAmount'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac5519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_265_monthly['Rolling_Mean'] = sales_265_monthly['NetAmount'].rolling(window=3).mean()\n",
    "sales_265_monthly['Rolling_Std'] = sales_265_monthly['NetAmount'].rolling(window=3).std()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(sales_265_monthly['DATE'], sales_265_monthly['NetAmount'], label='Original Sales', color='blue')\n",
    "plt.plot(sales_265_monthly['DATE'], sales_265_monthly['Rolling_Mean'], label='Rolling Mean', color='red')\n",
    "plt.plot(sales_265_monthly['DATE'], sales_265_monthly['Rolling_Std'], label='Rolling Std Dev', color='green')\n",
    "plt.title('Rolling Mean & Standard Deviation for SalesPersonCode 254')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3efed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_265_monthly['Diff_1'] = sales_265_monthly['NetAmount'].diff().dropna()\n",
    "\n",
    "print(\"\\nADF Test after First Differencing:\")\n",
    "adf_test(sales_265_monthly['Diff_1'].dropna())\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(sales_265_monthly['DATE'], sales_265_monthly['Diff_1'], marker='o', linestyle='-')\n",
    "plt.title('Differenced Monthly Sales for SalesPersonCode 265')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Differenced Sales')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b23e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade shap tensorflow keras numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ab612",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "data = pd.read_csv('2023-2024-Horana-cleaned.csv')\n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "data['NetAmount'] = data['NetAmount'].abs()  # Ensure all values are positive\n",
    "\n",
    "sales_265 = data[data['SalesPersonCode'] == '265']\n",
    "\n",
    "sales_265_monthly = sales_265.resample('M', on='DATE')['NetAmount'].sum().reset_index()\n",
    "sales_265_monthly.set_index('DATE', inplace=True)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_sales = scaler.fit_transform(sales_265_monthly[['NetAmount']])\n",
    "\n",
    "sales_265_monthly['NetAmount_Diff'] = sales_265_monthly['NetAmount'].diff().dropna()\n",
    "sales_265_monthly.dropna(inplace=True)\n",
    "\n",
    "scaler_diff = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_diff_data = scaler_diff.fit_transform(sales_265_monthly[['NetAmount_Diff']])\n",
    "\n",
    "def create_dataset(data, time_step=12):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step):\n",
    "        X.append(data[i:i + time_step, 0])\n",
    "        y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_step = 12  \n",
    "X, y = create_dataset(scaled_diff_data, time_step)\n",
    "\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(time_step, 1)),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=8, verbose=1)\n",
    "\n",
    "last_12_months = scaled_diff_data[-time_step:].reshape(1, time_step, 1)\n",
    "predicted_scaled_diff = model.predict(last_12_months)\n",
    "\n",
    "predicted_diff_value = scaler_diff.inverse_transform([[predicted_scaled_diff[0][0]]])[0][0]\n",
    "previous_month_sales = sales_265_monthly['NetAmount'].iloc[-1]\n",
    "predicted_sales = previous_month_sales + predicted_diff_value  # Reverse differencing\n",
    "\n",
    "actual_sales = sales_265_monthly['NetAmount'].iloc[-1]\n",
    "\n",
    "print(f\"Actual Sales for Last Month: {actual_sales:.2f}\")\n",
    "print(f\"Predicted Sales for Last Month: {predicted_sales:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(sales_265_monthly.index[-12:], sales_265_monthly['NetAmount'][-12:], label=\"Actual\", marker='o')\n",
    "plt.axhline(y=predicted_sales, color='r', linestyle='--', label=\"Predicted Last Month\")\n",
    "plt.legend()\n",
    "plt.title(\"Actual vs Predicted Monthly Sales for SalesPerson 254\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e04fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "data = pd.read_csv('2023-2024-Horana-cleaned.csv')\n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "data['NetAmount'] = data['NetAmount'].abs()  # Ensure all values are positive\n",
    "\n",
    "sales_265 = data[data['SalesPersonCode'] == '265']\n",
    "\n",
    "sales_265_monthly = sales_265.resample('M', on='DATE')['NetAmount'].sum().reset_index()\n",
    "sales_265_monthly.set_index('DATE', inplace=True)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_sales = scaler.fit_transform(sales_265_monthly[['NetAmount']])\n",
    "\n",
    "def create_dataset(data, time_step=12):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step):\n",
    "        X.append(data[i:i + time_step, 0])\n",
    "        y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_step = 12  \n",
    "X, y = create_dataset(scaled_sales, time_step)\n",
    "\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(X_test[0:5])\n",
    "print(y_test[0:5])\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(time_step, 1)),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=8, verbose=1)\n",
    "\n",
    "last_12_months = scaled_sales[-time_step:].reshape(1, time_step, 1)\n",
    "predicted_scaled = model.predict(last_12_months)\n",
    "\n",
    "predicted_sales = scaler.inverse_transform([[predicted_scaled[0][0]]])[0][0]\n",
    "\n",
    "actual_sales = sales_265_monthly['NetAmount'].iloc[-1]\n",
    "\n",
    "print(f\"Actual Sales for Last Month: {actual_sales:.2f}\")\n",
    "print(f\"Predicted Sales for Last Month: {predicted_sales:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(sales_265_monthly.index[-12:], sales_265_monthly['NetAmount'][-12:], label=\"Actual\", marker='o')\n",
    "plt.axhline(y=predicted_sales, color='r', linestyle='--', label=\"Predicted Last Month\")\n",
    "plt.legend()\n",
    "plt.title(\"Actual vs Predicted Monthly Sales for SalesPerson 254\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c80c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "print(\"SHAP Version:\", shap.__version__)\n",
    "\n",
    "try:\n",
    "    explainer = shap.DeepExplainer(model, X_train)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "except Exception as e:\n",
    "    print(\"DeepExplainer failed, switching to KernelExplainer...\")\n",
    "    background_samples = min(X_train.shape[0], 50)  # Avoid selecting more than available\n",
    "    background = X_train[np.random.choice(X_train.shape[0], background_samples, replace=False)]\n",
    "    explainer = shap.KernelExplainer(model.predict, background)\n",
    "\n",
    "    X_test_2d = X_test.reshape(X_test.shape[0], -1)  # Flatten timesteps and features into a single dimension\n",
    "    shap_values = explainer.shap_values(X_test_2d[:10])  # Use a small test set\n",
    "\n",
    "print(\"\\nSHAP Debugging Info\")\n",
    "print(\"SHAP Values Type:\", type(shap_values))\n",
    "print(\"SHAP Values Shape:\", np.array(shap_values).shape)\n",
    "print(\"X_test Shape:\", X_test.shape)\n",
    "\n",
    "try:\n",
    "    expected_value = explainer.expected_value\n",
    "    print(\"Expected Value:\", expected_value)\n",
    "except AttributeError:\n",
    "    print(\"Error: 'expected_value' not found in explainer.\")\n",
    "    expected_value = None\n",
    "\n",
    "print(\"\\nSample Check:\")\n",
    "print(\"SHAP Values First Sample:\", shap_values[0])\n",
    "print(\"X_test First Sample:\", X_test[0])\n",
    "\n",
    "if shap_values.shape == X_test_2d.shape:\n",
    "    shap.initjs()\n",
    "    shap.force_plot(expected_value, shap_values[0], X_test_2d[0], feature_names=[\"NetAmount\"])\n",
    "    shap.summary_plot(shap_values, X_test_2d, feature_names=[\"NetAmount\"])\n",
    "    shap.dependence_plot(0, shap_values, X_test_2d, feature_names=[\"NetAmount\"])\n",
    "else:\n",
    "    print(\"\\nERROR: Shapes still do not match. Debug needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caae0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a3842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import shap\n",
    "\n",
    "print(tf.__version__)\n",
    "print(shap.__version__)\n",
    "\n",
    "print(X_train.shape) \n",
    "print(X_test.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09685406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import shap\n",
    "\n",
    "data = pd.read_csv('2023-2024-Horana-cleaned.csv')\n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "data['NetAmount'] = data['NetAmount'].abs()  # Ensure all values are positive\n",
    "\n",
    "sales_265 = data[data['SalesPersonCode'] == '0']\n",
    "\n",
    "sales_265_monthly = sales_265.resample('M', on='DATE')['NetAmount'].sum().reset_index()\n",
    "sales_265_monthly.set_index('DATE', inplace=True)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_sales = scaler.fit_transform(sales_265_monthly[['NetAmount']])\n",
    "\n",
    "def create_dataset(data, time_step=12):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step):\n",
    "        X.append(data[i:i + time_step, 0])\n",
    "        y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_step = 12  \n",
    "X, y = create_dataset(scaled_sales, time_step)\n",
    "\n",
    "if len(X) == 0 or len(y) == 0:\n",
    "    raise ValueError(\"Not enough data to create sequences. Reduce time_step or use a larger dataset.\")\n",
    "\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "if len(X_train) == 0 or len(X_test) == 0:\n",
    "    raise ValueError(\"Not enough data to split into train and test sets. Reduce time_step or use a larger dataset.\")\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(time_step, 1)),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=8, verbose=1)\n",
    "\n",
    "last_12_months = scaled_sales[-time_step:].reshape(1, time_step, 1)\n",
    "predicted_scaled = model.predict(last_12_months)\n",
    "\n",
    "predicted_sales = scaler.inverse_transform([[predicted_scaled[0][0]]])[0][0]\n",
    "\n",
    "actual_sales = sales_265_monthly['NetAmount'].iloc[-1]\n",
    "\n",
    "print(f\"Actual Sales for Last Month: {actual_sales:.2f}\")\n",
    "print(f\"Predicted Sales for Last Month: {predicted_sales:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(sales_265_monthly.index[-12:], sales_265_monthly['NetAmount'][-12:], label=\"Actual\", marker='o')\n",
    "plt.axhline(y=predicted_sales, color='r', linestyle='--', label=\"Predicted Last Month\")\n",
    "plt.legend()\n",
    "plt.title(\"Actual vs Predicted Monthly Sales for SalesPerson 254\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "background_data = X_train[:100]  \n",
    "test_data = X_train[100:150]    \n",
    "\n",
    "if len(test_data) == 0:\n",
    "    raise ValueError(\"test_data is empty. Check the data splitting process.\")\n",
    "\n",
    "explainer = shap.GradientExplainer(model, background_data)\n",
    "\n",
    "shap_values = explainer.shap_values(test_data)\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "shap.summary_plot(shap_values, test_data, feature_names=[\"Sales\"], plot_type=\"bar\")\n",
    "\n",
    "sample_index = 0  \n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][sample_index], test_data[sample_index], feature_names=[\"Sales\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4926b96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import shap\n",
    "\n",
    "data = pd.read_csv('2023-2024-Horana-cleaned.csv')\n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "data['NetAmount'] = data['NetAmount'].abs()  # Ensure all values are positive\n",
    "\n",
    "sales_265 = data[data['SalesPersonCode'] == '265']\n",
    "\n",
    "sales_265_monthly = sales_265.resample('M', on='DATE')['NetAmount'].sum().reset_index()\n",
    "sales_265_monthly.set_index('DATE', inplace=True)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_sales = scaler.fit_transform(sales_265_monthly[['NetAmount']])\n",
    "\n",
    "def create_dataset(data, time_step=12):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step):\n",
    "        X.append(data[i:i + time_step, 0])\n",
    "        y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_step = 12  \n",
    "X, y = create_dataset(scaled_sales, time_step)\n",
    "\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(X_test[0:5])\n",
    "print(y_test[0:5])\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(time_step, 1)),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=8, verbose=1)\n",
    "\n",
    "last_12_months = scaled_sales[-time_step:].reshape(1, time_step, 1)\n",
    "predicted_scaled = model.predict(last_12_months)\n",
    "\n",
    "predicted_sales = scaler.inverse_transform([[predicted_scaled[0][0]]])[0][0]\n",
    "\n",
    "actual_sales = sales_265_monthly['NetAmount'].iloc[-1]\n",
    "\n",
    "print(f\"Actual Sales for Last Month: {actual_sales:.2f}\")\n",
    "print(f\"Predicted Sales for Last Month: {predicted_sales:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(sales_265_monthly.index[-12:], sales_265_monthly['NetAmount'][-12:], label=\"Actual\", marker='o')\n",
    "plt.axhline(y=predicted_sales, color='r', linestyle='--', label=\"Predicted Last Month\")\n",
    "plt.legend()\n",
    "plt.title(\"Actual vs Predicted Monthly Sales for SalesPerson 265\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "try:\n",
    "    background_samples_count = min(50, X_train.shape[0])  # Take 50 samples, or all if less than 50\n",
    "    \n",
    "    background_samples = X_train[np.random.choice(X_train.shape[0], background_samples_count, replace=False)]\n",
    "    \n",
    "    explainer = shap.KernelExplainer(model.predict, background_samples)\n",
    "    \n",
    "    X_test_2d = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    shap_values = explainer.shap_values(X_test_2d[:10])  # Use a small test set\n",
    "    \n",
    "    print(\"\\n==== SHAP Debugging Info ====\")\n",
    "    print(\"SHAP Values Type:\", type(shap_values))\n",
    "    print(\"SHAP Values Shape:\", np.array(shap_values).shape)\n",
    "    print(\"X_test Shape:\", X_test.shape)\n",
    "    \n",
    "    shap.initjs()\n",
    "    shap.force_plot(explainer.expected_value[0], shap_values[0], X_test_2d[0], feature_names=[f\"Month {i+1}\" for i in range(time_step)])\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with SHAP: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117b53df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "data = pd.read_csv('2023-2024-Horana-cleaned.csv')\n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "data['NetAmount'] = data['NetAmount'].abs()\n",
    "\n",
    "sales_265 = data[data['SalesPersonCode'] == '265']\n",
    "\n",
    "sales_265_monthly = sales_265.resample('ME', on='DATE')['NetAmount'].sum().reset_index()\n",
    "sales_265_monthly.set_index('DATE', inplace=True)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_sales = scaler.fit_transform(sales_265_monthly[['NetAmount']])\n",
    "\n",
    "def create_dataset(data, time_step=12):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step):\n",
    "        X.append(data[i:i + time_step, 0])\n",
    "        y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_step = 12  # 12 months for yearly trends\n",
    "X, y = create_dataset(scaled_sales, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Objective function for Optuna hyperparameter tuning.\"\"\"\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 2)\n",
    "    n_units = trial.suggest_int('n_units', 20, 100, step=10)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_units, return_sequences=(n_layers == 2), input_shape=(time_step, 1)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    if n_layers == 2:\n",
    "        model.add(LSTM(n_units))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=batch_size, verbose=0, callbacks=[early_stopping])\n",
    "    \n",
    "    loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(LSTM(best_params['n_units'], return_sequences=(best_params['n_layers'] == 2), input_shape=(time_step, 1)))\n",
    "final_model.add(Dropout(best_params['dropout_rate']))\n",
    "\n",
    "if best_params['n_layers'] == 2:\n",
    "    final_model.add(LSTM(best_params['n_units']))\n",
    "    final_model.add(Dropout(best_params['dropout_rate']))\n",
    "\n",
    "final_model.add(Dense(1))\n",
    "final_model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss='mean_squared_error')\n",
    "\n",
    "final_model.fit(X_train, y_train, epochs=50, batch_size=best_params['batch_size'], verbose=1, callbacks=[EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)])\n",
    "\n",
    "last_12_months = scaled_sales[-time_step:].reshape(1, time_step, 1)\n",
    "predicted_scaled = final_model.predict(last_12_months)\n",
    "predicted_sales = scaler.inverse_transform([[predicted_scaled[0][0]]])[0][0]\n",
    "\n",
    "actual_sales = sales_265_monthly['NetAmount'].iloc[-1]\n",
    "\n",
    "print(f\"Actual Sales for Last Month: {actual_sales:.2f}\")\n",
    "print(f\"Predicted Sales for Last Month: {predicted_sales:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(sales_265_monthly.index[-12:], sales_265_monthly['NetAmount'][-12:], label=\"Actual\", marker='o')\n",
    "plt.axhline(y=predicted_sales, color='r', linestyle='--', label=\"Predicted Last Month\")\n",
    "plt.legend()\n",
    "plt.title(\"Actual vs Predicted Monthly Sales for SalesPerson 265\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "try:\n",
    "    background_samples = X_train[np.random.choice(X_train.shape[0], min(50, X_train.shape[0]), replace=False)]\n",
    "\n",
    "    explainer = shap.GradientExplainer(final_model, background_samples)\n",
    "\n",
    "    X_test_shap = X_test[:3]  \n",
    "    print(f'>>>>>>>>>>:{X_test_shap}')\n",
    "\n",
    "    shap_values = explainer.shap_values(X_test_shap)\n",
    "\n",
    "    shap_values_reshaped = np.array(shap_values).reshape(3, 12)  \n",
    "\n",
    "    influential_reasons = []\n",
    "\n",
    "    for i in range(shap_values_reshaped.shape[0]):  \n",
    "        # print(f\"\\n==== top 3 influential reasons for prediction (sample {i+1}) ====\")\n",
    "\n",
    "        top_indices = np.argsort(np.abs(shap_values_reshaped[i]))[-3:][::-1]  \n",
    "\n",
    "        for rank, j in enumerate(top_indices, 1):  \n",
    "            contribution = shap_values_reshaped[i, j]\n",
    "            direction = \"increased\" if contribution > 0 else \"decreased\"\n",
    "            # message = f\"month {j+1} {direction} prediction by {abs(contribution):.4f}\"\n",
    "            # print(f\"top {rank}: {message}\")\n",
    "\n",
    "            influential_reasons.append((j+1, contribution))\n",
    "\n",
    "    top_overall = sorted(influential_reasons, key=lambda x: abs(x[1]), reverse=True)[:3]\n",
    "\n",
    "    print(\"\\n==== highest 3 influential reasons overall ====\")\n",
    "    for rank, (month, value) in enumerate(top_overall, 1):\n",
    "        direction = \"increased\" if value > 0 else \"decreased\"\n",
    "        print(f\"top {rank}: month {month} {direction} prediction by {abs(value):.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"error with shap: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98b63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "data = pd.read_csv('2023-2024-Horana-cleaned.csv')\n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "data['NetAmount'] = data['NetAmount'].abs()\n",
    "\n",
    "sales_265 = data[data['SalesPersonCode'] == '265']\n",
    "\n",
    "sales_265_monthly = sales_265.resample('ME', on='DATE')['NetAmount'].sum().reset_index()\n",
    "sales_265_monthly.set_index('DATE', inplace=True)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_sales = scaler.fit_transform(sales_265_monthly[['NetAmount']])\n",
    "\n",
    "def create_dataset(data, time_step=12):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step):\n",
    "        X.append(data[i:i + time_step, 0])\n",
    "        y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_step = 12 \n",
    "X, y = create_dataset(scaled_sales, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Objective function for Optuna hyperparameter tuning.\"\"\"\n",
    "    # Hyperparameter search space\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 2)\n",
    "    n_units = trial.suggest_int('n_units', 20, 100, step=10)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_units, return_sequences=(n_layers == 2), input_shape=(time_step, 1)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    if n_layers == 2:\n",
    "        model.add(LSTM(n_units))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=batch_size, verbose=0, callbacks=[early_stopping])\n",
    "    \n",
    "    loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(LSTM(best_params['n_units'], return_sequences=(best_params['n_layers'] == 2), input_shape=(time_step, 1)))\n",
    "final_model.add(Dropout(best_params['dropout_rate']))\n",
    "\n",
    "if best_params['n_layers'] == 2:\n",
    "    final_model.add(LSTM(best_params['n_units']))\n",
    "    final_model.add(Dropout(best_params['dropout_rate']))\n",
    "\n",
    "final_model.add(Dense(1))\n",
    "final_model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss='mean_squared_error')\n",
    "\n",
    "final_model.fit(X_train, y_train, epochs=50, batch_size=best_params['batch_size'], verbose=1, callbacks=[EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)])\n",
    "\n",
    "last_12_months = scaled_sales[-time_step:].reshape(1, time_step, 1)\n",
    "predicted_scaled = final_model.predict(last_12_months)\n",
    "predicted_sales = scaler.inverse_transform([[predicted_scaled[0][0]]])[0][0]\n",
    "\n",
    "next_month = sales_265_monthly.index[-1] + pd.DateOffset(months=1)\n",
    "\n",
    "217print(f\"predicted sales for {next_month.strftime('%b %Y')}: {predicted_sales:.2f}\")\n",
    "\n",
    "# plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(sales_265_monthly.index[-12:], sales_265_monthly['NetAmount'][-12:], label=\"actual\", marker='o')\n",
    "plt.scatter(next_month, predicted_sales, color='red', marker='x', label=\"predicted next month\")\n",
    "plt.legend()\n",
    "plt.title(\"actual vs predicted monthly sales for salesperson 265\")\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"sales\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    background_samples = X_train[np.random.choice(X_train.shape[0], min(50, X_train.shape[0]), replace=False)]\n",
    "\n",
    "    explainer = shap.GradientExplainer(final_model, background_samples)\n",
    "\n",
    "    X_test_shap = X_test[:3]  \n",
    "    # print(f'>>>>>>>>>>:{X_test_shap}')\n",
    "\n",
    "    shap_values = explainer.shap_values(X_test_shap)\n",
    "\n",
    "    shap_values_reshaped = np.array(shap_values).reshape(3, 12)  \n",
    "\n",
    "    influential_reasons = []\n",
    "\n",
    "    for i in range(shap_values_reshaped.shape[0]):  \n",
    "\n",
    "        top_indices = np.argsort(np.abs(shap_values_reshaped[i]))[-3:][::-1]  \n",
    "\n",
    "        for rank, j in enumerate(top_indices, 1):  \n",
    "            contribution = shap_values_reshaped[i, j]\n",
    "            direction = \"increased\" if contribution > 0 else \"decreased\"\n",
    "\n",
    "            influential_reasons.append((j+1, contribution))\n",
    "\n",
    "    top_overall = sorted(influential_reasons, key=lambda x: abs(x[1]), reverse=True)[:3]\n",
    "\n",
    "    print(\"\\n==== highest 3 influential reasons overall ====\")\n",
    "    for rank, (month, value) in enumerate(top_overall, 1):\n",
    "        direction = \"increased\" if value > 0 else \"decreased\"\n",
    "        print(f\"top {rank}: month {month} {direction} prediction by {abs(value):.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"error with shap: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db00129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "data = pd.read_csv('2023-2024-Horana-cleaned.csv')\n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "data['NetAmount'] = data['NetAmount'].abs()\n",
    "\n",
    "kpi_data = pd.read_csv('KPI.csv')\n",
    "kpi_data['Year-Month'] = pd.to_datetime(kpi_data['Year-Month'], format='%Y-%m')\n",
    "kpi_data['Person Code'] = kpi_data['Person Code'].astype(str)\n",
    "\n",
    "sales_265 = data[data['SalesPersonCode'] == '265']\n",
    "sales_265_monthly = sales_265.resample('ME', on='DATE')['NetAmount'].sum().reset_index()\n",
    "sales_265_monthly['Year-Month'] = sales_265_monthly['DATE'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "merged_data = sales_265_monthly.merge(kpi_data, on=['Year-Month'], how='left')\n",
    "\n",
    "features = ['NetAmount', 'Punctuality', 'Absenteeism', 'Maintaining Env', 'Sales Perf', 'Cust. Sat']\n",
    "merged_data[features] = merged_data[features].apply(pd.to_numeric)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(merged_data[features])\n",
    "\n",
    "def create_dataset(data, time_step=12):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step):\n",
    "        X.append(data[i:i + time_step])\n",
    "        y.append(data[i + time_step, 0])  # NetAmount as target\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_step = 12\n",
    "X, y = create_dataset(scaled_data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], X.shape[2])\n",
    "\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "def objective(trial):\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 2)\n",
    "    n_units = trial.suggest_int('n_units', 20, 100, step=10)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_units, return_sequences=(n_layers == 2), input_shape=(time_step, X.shape[2])))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    if n_layers == 2:\n",
    "        model.add(LSTM(n_units))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=batch_size, verbose=0, callbacks=[early_stopping])\n",
    "    \n",
    "    loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(LSTM(best_params['n_units'], return_sequences=(best_params['n_layers'] == 2), input_shape=(time_step, X.shape[2])))\n",
    "final_model.add(Dropout(best_params['dropout_rate']))\n",
    "\n",
    "if best_params['n_layers'] == 2:\n",
    "    final_model.add(LSTM(best_params['n_units']))\n",
    "    final_model.add(Dropout(best_params['dropout_rate']))\n",
    "\n",
    "final_model.add(Dense(1))\n",
    "final_model.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss='mean_squared_error')\n",
    "\n",
    "final_model.fit(X_train, y_train, epochs=50, batch_size=best_params['batch_size'], verbose=1, callbacks=[EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)])\n",
    "\n",
    "last_12_months = scaled_data[-time_step:].reshape(1, time_step, scaled_data.shape[1])\n",
    "predicted_scaled = final_model.predict(last_12_months)\n",
    "predicted_sales = scaler.inverse_transform([[predicted_scaled[0][0]] + [0] * (scaled_data.shape[1] - 1)])[0][0]\n",
    "\n",
    "next_month = merged_data['Year-Month'].iloc[-1] + pd.DateOffset(months=1)\n",
    "print(f\"Predicted sales for {next_month.strftime('%b %Y')}: {predicted_sales:.2f}\")\n",
    "\n",
    "try:\n",
    "    background_samples = X_train[np.random.choice(X_train.shape[0], min(50, X_train.shape[0]), replace=False)]\n",
    "    explainer = shap.GradientExplainer(final_model, background_samples)\n",
    "    X_test_shap = X_test[:3]\n",
    "    shap_values = explainer.shap_values(X_test_shap)\n",
    "    shap_values_reshaped = np.array(shap_values).reshape(3, time_step, X.shape[2])\n",
    "    influential_reasons = {}\n",
    "    for i in range(shap_values_reshaped.shape[0]):\n",
    "        for j in range(1, X.shape[2]):  \n",
    "            month_index = np.argmax(np.abs(shap_values_reshaped[i, :, j]))\n",
    "            month = merged_data['Year-Month'].iloc[train_size + month_index]\n",
    "            key = (month, features[j])\n",
    "            if key not in influential_reasons or abs(shap_values_reshaped[i, month_index, j]) > abs(influential_reasons[key]):\n",
    "                influential_reasons[key] = shap_values_reshaped[i, month_index, j]\n",
    "    top_overall = sorted(influential_reasons.items(), key=lambda x: abs(x[1]), reverse=True)[:3]\n",
    "    print(\"\\n==== Highest 3 Influential KPI Reasons Overall ====\")\n",
    "    for rank, ((month, kpi), value) in enumerate(top_overall, 1):\n",
    "        direction = \"increased\" if value > 0 else \"decreased\"\n",
    "        print(f\"Top {rank}: ({month.strftime('%Y-%m')}) {kpi} {direction} prediction by {abs(value):.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with SHAP: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e6edbd-66bd-4f7b-8ca8-60d6584efd0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
